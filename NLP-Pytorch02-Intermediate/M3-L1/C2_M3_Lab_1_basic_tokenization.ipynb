{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Welcome to your first hands-on lab for Natural Language Processing (NLP)! \n",
    "Unlike images, which are naturally numerical arrays, text is a sequence of symbols that machines don't inherently understand. Before you can perform tasks like sentiment analysis or translation, you must first convert your text into a format a model can process.\n",
    "\n",
    "**Tokenization** is an important first step in any NLP workflow, converting raw text into meaningful units called **tokens**.\n",
    "These tokens are building blocks used by models, such as BERT, to generate word embeddings - dense vector representations capturing semantic meaning.\n",
    "\n",
    "This lab provides a practical look into this fundamental process. \n",
    "You will explore tokenization by comparing a manual, from-scratch approach with the use of a modern, pre-trained tool.\n",
    "\n",
    "Specifically, you'll learn to:\n",
    "* Build a simple tokenizer from scratch to understand the core mechanics, creating a vocabulary where each unique word maps to a numerical ID.\n",
    "* Use a powerful, pre-trained BERT tokenizer from the popular Hugging Face library to see how professionals handle this task efficiently.\n",
    "* Understand why matching tokenizers to models is critical and use `AutoTokenizer` as a best practice for ensuring compatibility.\n",
    "* Observe how this advanced tool automatically handles challenges like out-of-vocabulary (OOV) words by breaking them into **subword tokens**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, AutoTokenizer\n",
    "\n",
    "import helper_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Tokenization: Building a Vocabulary\n",
    "\n",
    "* Define a list of sample `sentences`.\n",
    "* Implement `tokenize` function that converts input `text` to lowercase and splits it into individual words (tokens) based on whitespace.\n",
    "* Implement `build_vocab` function that takes a list of `sentences`, tokenizes them, and creates a `vocab` (vocabulary).\n",
    "    * Each unique word encountered is added to the vocabulary and assigned a unique numerical ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1huRfbAfTfN",
    "outputId": "eff8b09f-cba4-4c61-aced-b19c7f830c93",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Index: {'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat'\n",
    "]\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(text):\n",
    "  # Lowercase the text and split by whitespace\n",
    "  return text.lower().split()\n",
    "\n",
    "# Build the vocabulary\n",
    "def build_vocab(sentences):\n",
    "    vocab = {}\n",
    "    # Iterate through each sentence.\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the current sentence\n",
    "        tokens = tokenize(sentence)\n",
    "        # Iterate through each token in the sentence\n",
    "        for token in tokens:\n",
    "            # If the token is not already in the vocabulary\n",
    "            if token not in vocab:\n",
    "                # Add the token to the vocabulary and assign it a unique integer ID\n",
    "                # IDs start from 1; 0 can be reserved for padding.\n",
    "                vocab[token] = len(vocab) + 1\n",
    "    return vocab\n",
    "\n",
    "# Create the vocabulary index\n",
    "vocab = build_vocab(sentences)\n",
    "\n",
    "print(\"Vocabulary Index:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pre-trained BERT Tokenizer\n",
    "\n",
    "* Initialize the `BertTokenizerFast` by loading the pre-trained [bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) model directly from Hugging Face.\n",
    "\n",
    "**Note**: In this notebook environment, the model has been saved and is being loaded locally:\n",
    "\n",
    "```python\n",
    "local_tokenizer_path = \"./bert_tokenizer_local\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(local_tokenizer_path)\n",
    "```\n",
    "\n",
    "If you were to run this notebook elsewhere, you would initialize it as:\n",
    "     \n",
    "```python\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "```\n",
    "\n",
    "* Use the initialized `tokenizer` to process the `sentences`, creating `encoded_inputs`.\n",
    "    * `padding=True` ensures all output sequences have the same length.\n",
    "    * `truncation=True` cuts sequences that are longer than the model's maximum input length.\n",
    "    * `return_tensors='pt'` specifies that the output should be PyTorch tensors.\n",
    "* Convert the `input_ids` (numerical representations) from `encoded_inputs` back into their string token representations for easier inspection.\n",
    "    * These may include special tokens like `[CLS]` and `[SEP]`.\n",
    "* Retrieve the entire vocabulary (word-to-ID mapping) used by the BERT tokenizer using `tokenizer.get_vocab()`.\n",
    "* Print the `input_ids` (token IDs) generated by the tokenizer for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [['[CLS]', 'i', 'love', 'my', 'dog', '[SEP]'], ['[CLS]', 'i', 'love', 'my', 'cat', '[SEP]']]\n",
      "\n",
      "Token IDs: tensor([[ 101, 1045, 2293, 2026, 3899,  102],\n",
      "        [ 101, 1045, 2293, 2026, 4937,  102]])\n",
      "\n",
      "--- Unique Token to ID Mappings (for these sentences) ---\n",
      "[CLS]\t-->\t101\n",
      "[SEP]\t-->\t102\n",
      "i\t-->\t1045\n",
      "my\t-->\t2026\n",
      "love\t-->\t2293\n",
      "dog\t-->\t3899\n",
      "cat\t-->\t4937\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat'\n",
    "]\n",
    "\n",
    "# Define the local directory where the tokenizer is saved\n",
    "local_tokenizer_path = \"./bert_tokenizer_local\"\n",
    "\n",
    "# Initialize the tokenizer from the local directory\n",
    "tokenizer = BertTokenizerFast.from_pretrained(local_tokenizer_path)\n",
    "\n",
    "# Tokenize the sentences and encode them\n",
    "encoded_inputs = tokenizer(sentences, padding=True, \n",
    "                           truncation=True, return_tensors='pt')\n",
    "\n",
    "# To see the tokens for each input (helpful for understanding the output)\n",
    "tokens = [tokenizer.convert_ids_to_tokens(ids)\n",
    "          for ids in encoded_inputs[\"input_ids\"]]\n",
    "\n",
    "# Get the model's vocabulary (mapping from tokens to IDs)\n",
    "word_index = tokenizer.get_vocab() # For BertTokenizerFast, get_vocab() returns the vocab\n",
    "\n",
    "# Print the human-readable `tokens` for each sentence\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"\\nToken IDs:\", encoded_inputs['input_ids'])\n",
    "\n",
    "# Print unique tokens from your sentences mapped to their unique IDs \n",
    "helper_utils.print_unique_token_id_mappings(tokens, encoded_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark on Model Compatibility**\n",
    "It is worth emphasizing that in NLP, tokenizers are not one-size-fits-all tools. Each tokenizer is specifically designed to work with a particular model.\n",
    "The `bert-base-uncased` tokenizer, for example, is designed to format text in the exact way the BERT model was trained to understand it. \n",
    "This includes its specific vocabulary, rules for splitting words, and the use of special tokens like `[CLS]` and `[SEP]`.\n",
    "\n",
    "*Using the tokenizer that matches your model ensures the input format is exactly what the model expects.* \n",
    "Mismatching a model and tokenizer can lead to poor performance or errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `AutoTokenizer`\n",
    "\n",
    "While using a specific class like `BertTokenizerFast` works perfectly, the Hugging Face `transformers` library offers a convenient and robust solution: `AutoTokenizer`.\n",
    "\n",
    "The `AutoTokenizer` class is a smart wrapper that automatically detects and loads the correct tokenizer class for any given model checkpoint. \n",
    "Instead of you needing to remember whether a model requires `BertTokenizerFast`, `GPT2Tokenizer`, or another specific class, `AutoTokenizer.from_pretrained()` handles it for you.\n",
    "\n",
    "This simplifies your code and, more importantly, prevents potential mismatches between your model and its tokenizer. \n",
    "\n",
    "* Initialize the `AutoTokenizer` by loading the same pre-trained `bert-base-uncased` model.\n",
    "* Use the `AutoTokenizer` to process the `sentences`, creating `encoded_inputs_auto`.\n",
    "    * The same parameters (`padding`, `truncation`, `return_tensors`) are used to ensure consistent output formatting.\n",
    "* Convert the `input_ids` from `encoded_inputs_auto` back into their string token representations for inspection.\n",
    "* Print the `input_ids` generated by the `AutoTokenizer` for the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local directory where the tokenizer is saved\n",
    "local_tokenizer_path = \"./bert_tokenizer_local\"\n",
    "\n",
    "# Initialize the tokenizer using the AutoTokenizer class\n",
    "# This automatically loads the correct tokenizer (BertTokenizerFast in this case)\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [['[CLS]', 'i', 'love', 'my', 'dog', '[SEP]'], ['[CLS]', 'i', 'love', 'my', 'cat', '[SEP]']]\n",
      "\n",
      "Token IDs: tensor([[ 101, 1045, 2293, 2026, 3899,  102],\n",
      "        [ 101, 1045, 2293, 2026, 4937,  102]])\n",
      "\n",
      "--- Unique Token to ID Mappings (for these sentences) ---\n",
      "[CLS]\t-->\t101\n",
      "[SEP]\t-->\t102\n",
      "i\t-->\t1045\n",
      "my\t-->\t2026\n",
      "love\t-->\t2293\n",
      "dog\t-->\t3899\n",
      "cat\t-->\t4937\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat'\n",
    "]\n",
    "\n",
    "# Tokenize the sentences and encode them\n",
    "encoded_inputs = tokenizer(sentences, padding=True, \n",
    "                           truncation=True, return_tensors='pt')\n",
    "\n",
    "# To see the tokens for each input (helpful for understanding the output)\n",
    "tokens = [tokenizer.convert_ids_to_tokens(ids)\n",
    "          for ids in encoded_inputs[\"input_ids\"]]\n",
    "\n",
    "# Get the model's vocabulary (mapping from tokens to IDs)\n",
    "word_index = tokenizer.get_vocab() \n",
    "\n",
    "# Print the human-readable `tokens` for each sentence\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"\\nToken IDs:\", encoded_inputs['input_ids'])\n",
    "\n",
    "# Print unique tokens from your sentences mapped to their unique IDs \n",
    "helper_utils.print_unique_token_id_mappings(tokens, encoded_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Try It With Your Own Sentences\n",
    "\n",
    "You've seen how the pre-trained BERT tokenizer processed the example sentences. Now, it's your turn to experiment! Use the code cell below to input your own sentences and observe how they are tokenized\n",
    "\n",
    "Test sentences of different lengths. For example,\n",
    "```python\n",
    "sentences = [\n",
    "    'I love my red dog',\n",
    "    'I love my cat'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add your sentence(s) here\n",
    "sentences = [\n",
    "    \"What are OOV words? Words not in the tokenizer's built-in dictionary (e.g., many proper names).\",\n",
    "    \"The next code cell is all set to take these sentences and process them using the tokenizer. It will then print out how your sentences have been converted into 'Tokens' and their corresponding 'Token IDs'.\",\n",
    "    # \"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell is all set to take these sentences and process them using the tokenizer. It will then print out how your sentences have been converted into 'Tokens' and their corresponding 'Token IDs'.\n",
    "\n",
    "**Before you run it, here's something to look out for:** If you've included any words that are particularly unique or specific (like names of local people, specific places, or less common nouns), pay close attention to how these words appear in the 'Tokens' list after you run the cell. You might notice they are handled in a distinct way.\n",
    "\n",
    "An explanation for this behavior, especially concerning these kinds of words, will be provided in the section below the output, under the heading **\"Out-of-Vocabulary\" Words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [['[CLS]', 'what', 'are', 'o', '##ov', 'words', '?', 'words', 'not', 'in', 'the', 'token', '##izer', \"'\", 's', 'built', '-', 'in', 'dictionary', '(', 'e', '.', 'g', '.', ',', 'many', 'proper', 'names', ')', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'], ['[CLS]', 'the', 'next', 'code', 'cell', 'is', 'all', 'set', 'to', 'take', 'these', 'sentences', 'and', 'process', 'them', 'using', 'the', 'token', '##izer', '.', 'it', 'will', 'then', 'print', 'out', 'how', 'your', 'sentences', 'have', 'been', 'converted', 'into', \"'\", 'token', '##s', \"'\", 'and', 'their', 'corresponding', \"'\", 'token', 'id', '##s', \"'\", '.', '[SEP]']]\n",
      "\n",
      "Token IDs: tensor([[  101,  2054,  2024,  1051,  4492,  2616,  1029,  2616,  2025,  1999,\n",
      "          1996, 19204, 17629,  1005,  1055,  2328,  1011,  1999,  9206,  1006,\n",
      "          1041,  1012,  1043,  1012,  1010,  2116,  5372,  3415,  1007,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2279,  3642,  3526,  2003,  2035,  2275,  2000,  2202,\n",
      "          2122, 11746,  1998,  2832,  2068,  2478,  1996, 19204, 17629,  1012,\n",
      "          2009,  2097,  2059,  6140,  2041,  2129,  2115, 11746,  2031,  2042,\n",
      "          4991,  2046,  1005, 19204,  2015,  1005,  1998,  2037,  7978,  1005,\n",
      "         19204,  8909,  2015,  1005,  1012,   102]])\n",
      "\n",
      "--- Unique Token to ID Mappings (for these sentences) ---\n",
      "[PAD]\t-->\t0\n",
      "[CLS]\t-->\t101\n",
      "[SEP]\t-->\t102\n",
      "'\t-->\t1005\n",
      "(\t-->\t1006\n",
      ")\t-->\t1007\n",
      ",\t-->\t1010\n",
      "-\t-->\t1011\n",
      ".\t-->\t1012\n",
      "?\t-->\t1029\n",
      "e\t-->\t1041\n",
      "g\t-->\t1043\n",
      "o\t-->\t1051\n",
      "s\t-->\t1055\n",
      "the\t-->\t1996\n",
      "and\t-->\t1998\n",
      "in\t-->\t1999\n",
      "to\t-->\t2000\n",
      "is\t-->\t2003\n",
      "it\t-->\t2009\n",
      "##s\t-->\t2015\n",
      "are\t-->\t2024\n",
      "not\t-->\t2025\n",
      "have\t-->\t2031\n",
      "all\t-->\t2035\n",
      "their\t-->\t2037\n",
      "out\t-->\t2041\n",
      "been\t-->\t2042\n",
      "into\t-->\t2046\n",
      "what\t-->\t2054\n",
      "then\t-->\t2059\n",
      "them\t-->\t2068\n",
      "will\t-->\t2097\n",
      "your\t-->\t2115\n",
      "many\t-->\t2116\n",
      "these\t-->\t2122\n",
      "how\t-->\t2129\n",
      "take\t-->\t2202\n",
      "set\t-->\t2275\n",
      "next\t-->\t2279\n",
      "built\t-->\t2328\n",
      "using\t-->\t2478\n",
      "words\t-->\t2616\n",
      "process\t-->\t2832\n",
      "names\t-->\t3415\n",
      "cell\t-->\t3526\n",
      "code\t-->\t3642\n",
      "##ov\t-->\t4492\n",
      "converted\t-->\t4991\n",
      "proper\t-->\t5372\n",
      "print\t-->\t6140\n",
      "corresponding\t-->\t7978\n",
      "id\t-->\t8909\n",
      "dictionary\t-->\t9206\n",
      "sentences\t-->\t11746\n",
      "##izer\t-->\t17629\n",
      "token\t-->\t19204\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the sentences and encode them\n",
    "encoded_inputs = tokenizer(sentences, padding=True, \n",
    "                           truncation=True, return_tensors='pt')\n",
    "\n",
    "# To see the tokens for each input (helpful for understanding the output)\n",
    "tokens = [tokenizer.convert_ids_to_tokens(ids)\n",
    "          for ids in encoded_inputs[\"input_ids\"]]\n",
    "\n",
    "# Get the model's vocabulary (mapping from tokens to IDs)\n",
    "word_index = tokenizer.get_vocab()\n",
    "\n",
    "# Print the human-readable `tokens` for each sentence\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "print(\"\\nToken IDs:\", encoded_inputs['input_ids'])\n",
    "\n",
    "# Print unique tokens from your sentences mapped to their unique IDs \n",
    "helper_utils.print_unique_token_id_mappings(tokens, encoded_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Out-of-Vocabulary\" (OOV) Words\n",
    "\n",
    "You might have seen some words in your sentences (especially unique names or local terms) break into smaller pieces when tokenized. This is expected.\n",
    "\n",
    "* **What are OOV words?** Words not in the tokenizer's built-in dictionary (e.g., many proper names).\n",
    "* **How are they handled?** The tokenizer splits OOV words into smaller, known sub-word parts.\n",
    "* **What does \"##\" mean?** A sub-word starting with \"##\" (like ##bs) attaches to the previous piece to form the original word. It's not a new word itself.\n",
    "\n",
    "**Example**:\n",
    "If a name like `\"Mubsi\"` is OOV, it might become ['mu', '##bs', '##i']. This means \"mu\" + \"bs\" + \"i\" are combined to represent \"Mubsi\".\n",
    "\n",
    "**Why does this happen?** This \"subword tokenization\" allows the tokenizer to handle any word, even if it's rare or new, ensuring no word is truly \"unknown.\"\n",
    "\n",
    "To see this in action, use the `tokenizer` on the `oov_words` and check the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Subword Tokenization Example ---\n",
      "Original word: 'Tokenization'\n",
      "Subword tokens: ['token', '##ization']\n",
      "\n",
      "Original word: 'HuggingFace'\n",
      "Subword tokens: ['hugging', '##face']\n",
      "\n",
      "Original word: 'unintelligible'\n",
      "Subword tokens: ['un', '##int', '##elli', '##gible']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A list of words that are likely \"Out-of-Vocabulary\" (OOV)\n",
    "oov_words = [\"Tokenization\", \"HuggingFace\", \"unintelligible\"]\n",
    "\n",
    "print(\"--- Subword Tokenization Example ---\")\n",
    "\n",
    "# Iterate through the words and show how they are tokenized\n",
    "for word in oov_words:\n",
    "    # The .tokenize() method is a direct way to see the subword breakdown\n",
    "    subwords = tokenizer.tokenize(word)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Original word: '{word}'\")\n",
    "    print(f\"Subword tokens: {subwords}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations on completing the lab! You have successfully transformed raw text into structured, numerical tensors that a deep learning model can understand.\n",
    "\n",
    "You started by building a vocabulary manually, tokenizing sentences, and assigning a unique ID to each word. This foundational exercise highlights the core challenge: every unique word needs a numerical representation, and your vocabulary can quickly become massive and difficult to manage.\n",
    "\n",
    "Then, you saw the modern approach: using a pre-trained tokenizer. With just a few lines of code, it handles the entire preprocessing pipelineâ€”from splitting words and adding special tokens like `[CLS]` and `[SEP]` to padding and truncation. You also saw how **subword tokenization** elegantly solves the out-of-vocabulary problem, ensuring that no word is ever truly \"unknown\" to the model.\n",
    "\n",
    "The key takeaway is that the tokenizer and its corresponding model are tightly coupled. The BERT tokenizer formats the text in the exact way the BERT model was trained to understand it, which is essential for achieving state-of-the-art performance. Using tools like `AutoTokenizer` simplifies this process, guaranteeing you always use the correct tokenizer for your chosen model. Now that you can reliably convert any text into model-ready tensors, you are prepared to move on to the next stage: using these tensors to build and train powerful NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
