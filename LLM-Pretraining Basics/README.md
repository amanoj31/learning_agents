Course: Pretraining LLMs - https://www.coursera.org/learn/pretraining-llms/home/welcome

LLM-Pretraining Basics - Autogenerated Summary

Mapping of lessons to topics and key teaching points:

- `L1/Lesson_1.ipynb` : Lesson 1 — Why Pretraining?
  - Motivates pretraining, shows loading pretrained models and sampling text; compares general / finetuned / continued-pretraining variants.
  - Hands-on: load small models (`TinySolar-248m-4k`), run generation with `TextStreamer` and compare pretrained vs fine-tuned variants.
  - Technical details & techniques: `transformers` model/tokenizer loading, `AutoModelForCausalLM`, `TextStreamer`, device mapping and bfloat16 usage, prompt/temperature settings.
  - Notebook: `L1/Lesson_1.ipynb`

- `L2/Lesson_2.ipynb` : Lesson 2 — Data Preparation
  - Prepare pretraining data: download/collect datasets (Hugging Face subsets, scraped code), column selection, cleaning and filtering steps.
  - Hands-on: create `Dataset`, filter short examples, remove repetitions, deduplicate, apply language-quality filters, save as Parquet.
  - Technical details & techniques: `datasets` API, filtering functions, fastText language detection, parquet export for large-scale pipelines.
  - Notebook: `L2/Lesson_2.ipynb`

- `L3/Lesson_3.ipynb` : Lesson 3 — Data Packaging
  - Tokenization and packing: load packed parquet, shard datasets, tokenize with model tokenizer and pack `input_ids` into fixed-length sequences.
  - Hands-on: write `tokenization` mapping, concatenate & reshape `input_ids`, and export packaged training shards.
  - Technical details & techniques: `AutoTokenizer`, tokenization helpers, dataset `map()` and `shard()`, packing logic for fixed `max_seq_length`.
  - Notebook: `L3/Lesson_3.ipynb`

- `L4/Lesson_4.ipynb` : Lesson 4 — Preparing Your Model for Training
  - Model configuration, weight initialization strategies (random init, reuse, downscale/upscale), model inspection and inference sanity checks.
  - Hands-on: `LlamaConfig` tuning, layer removal (downscale) and depth upscaling workflows, saving initialized checkpoints.
  - Technical details & techniques: `LlamaConfig`, `AutoModelForCausalLM`, parameter counting, layer copying for upscaling, memory management and GC for large models.
  - Notebook: `L4/Lesson_4.ipynb`

- `L5/Lesson_5.ipynb` : Lesson 5 — Model Training
  - Training setup: dataset wrappers for the trainer, custom `TrainingArguments`, building `Trainer` and monitoring training (callbacks, logging).
  - Hands-on: create `CustomDataset`, `CustomArguments`, run small-scale training with `Trainer`, and test intermediate checkpoints.
  - Technical details & techniques: Hugging Face `Trainer`, gradient checkpointing, `bf16` training flags, checkpointing and inference evaluation.
  - Notebook: `L5/Lesson_5.ipynb`

- `L6/Lesson_6.ipynb` : Lesson 6 — Model Evaluation
  - Evaluation tooling: using LM Evaluation Harness, TruthfulQA and Hugging Face leaderboard-style evaluation workflows.
  - Hands-on: run `lm_eval` example tasks, examples for leaderboard evaluation workflows and scripting eval runs.
  - Technical details & techniques: LM Evaluation Harness integration, running benchmark suites, scripting multi-task eval runs.
  - Notebook: `L6/Lesson_6.ipynb`

Notes:
- These lessons rely on large models and datasets; many cells assume local `./models/` and API access. 
