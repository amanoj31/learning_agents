Course: Quantization Fundamentals with Hugging Face : https://www.coursera.org/programs/school-of-data-management-sieb0/learn/quantization-fundamentals

LLM-Quantization Basics - Autogenerated Summary

Mapping of lessons to topics and key teaching points:

- `L1/L2_data_types.ipynb` : Lesson 2 — Data Types and Sizes
  - Overview of numeric types used for model parameters (int8, fp16, bf16, fp32) and their memory/precision tradeoffs.
  - Hands-on: inspect `torch.iinfo`/`torch.finfo`, downcasting examples and dot-product comparison between fp32 and bf16.
  - Technical details & techniques: `torch` dtypes, `to(dtype=...)` downcasting, numeric precision effects and performance/memory tradeoffs.
  - Notebook: `L1/L2_data_types.ipynb`

- `L2/L3_models_with_different_data_types.ipynb` : Lesson 3 — Models with Different Data Types
  - Load models and cast parameters between `float32`, `float16`, and `bfloat16`, compare outputs and memory footprint.
  - Hands-on: use a dummy model and a real example (`Salesforce/blip-image-captioning-base`) to compare generation and memory usage.
  - Technical details & techniques: model casting (`.half()`, `.to(torch.bfloat16)`), deep copy for dtype experiments, memory footprint inspection and evaluation.
  - Notebook: `L2/L3_models_with_different_data_types.ipynb`

- `L3/L4_quantization_theory.ipynb` : Lesson 4 — Quantization Theory
  - Introduces linear quantization (8-bit) via the `quanto` library, freezing quantized weights, and running inference on quantized models.
  - Hands-on: quantize (int8) and freeze smaller models (T5-Flan) as a demo, compare quantized inference and model size reductions.
  - Technical details & techniques: `quanto.quantize`, `freeze()`, tradeoffs vs downcasting, and practical hardware constraints for classroom environments.
  - Notebook: `L3/L4_quantization_theory.ipynb`

Notes:
- These lessons demonstrate both simple downcasting (bf16) and more advanced linear quantization (int8) and explain when each approach is appropriate.
